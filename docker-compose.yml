services:
  # -------------------- ZOOKEEPER --------------------
  zookeeper:
    image: confluentinc/cp-zookeeper:7.5.0
    container_name: zookeeper
    hostname: zookeeper
    mem_limit: 512m
    ports:
      - "${ZOOKEEPER_CLIENT_PORT}:${ZOOKEEPER_CLIENT_PORT}"
    environment:
      ZOOKEEPER_CLIENT_PORT: ${ZOOKEEPER_CLIENT_PORT}
      ZOOKEEPER_TICK_TIME: ${ZOOKEEPER_TICK_TIME}
    volumes:
      - ./kafka/zookeeper_data/data:/var/lib/zookeeper/data
      - ./kafka/zookeeper_data/log:/var/lib/zookeeper/log

  # -------------------- KAFKA --------------------
  kafka:
    image: confluentinc/cp-kafka:7.5.0
    container_name: kafka
    hostname: kafka
    mem_limit: 4g
    ports:
      - "9092:9092"
      - "9093:9093"
    environment:
      KAFKA_BROKER_ID: ${KAFKA_BROKER_ID}
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: ${KAFKA_LISTENER_SECURITY_PROTOCOL_MAP}
      KAFKA_ZOOKEEPER_CONNECT: ${KAFKA_ZOOKEEPER_CONNECT}
      KAFKA_ADVERTISED_LISTENERS: ${KAFKA_ADVERTISED_LISTENERS}
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: ${KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR}
      KAFKA_NUM_PARTITIONS: ${KAFKA_NUM_PARTITIONS}
    volumes:
      - ./kafka/kafka_broker/kafka_store:/var/lib/kafka/data
      - ./kafka/kafka_etc/kafak_secrets:/etc/kafka/secrets
    depends_on:
      - zookeeper

  # -------------------- KAFDROP --------------------
  kafdrop:
    image: obsidiandynamics/kafdrop:latest
    container_name: kafdrop
    hostname: kafdrop
    mem_limit: 256m
    ports:
      - "19000:9000"
    environment:
      KAFKA_BROKERCONNECT: ${KAFKA_BROKERCONNECT}
      JVM_OPTS: ${JVM_OPTS}
      SERVER_SERVLET_CONTEXTPATH: ${SERVER_SERVLET_CONTEXTPATH}
    depends_on:
      - kafka

  # -------------------- SCHEMA REGISTRY --------------------
  schema-registry:
    image: confluentinc/cp-schema-registry:5.5.3
    container_name: schema-registry
    environment:
      SCHEMA_REGISTRY_KAFKASTORE_CONNECTION_URL: ${SCHEMA_REGISTRY_KAFKASTORE_CONNECTION_URL}
      SCHEMA_REGISTRY_HOST_NAME: ${SCHEMA_REGISTRY_HOST_NAME}
      SCHEMA_REGISTRY_LISTENERS: ${SCHEMA_REGISTRY_LISTENERS}
    ports:
      - "8085:8081"
    depends_on:
      - zookeeper
      - kafka

  # -------------------- DEBEZIUM --------------------
  debezium:
    image: debezium/connect:1.4
    container_name: debezium
    environment:
      BOOTSTRAP_SERVERS: ${BOOTSTRAP_SERVERS}
      GROUP_ID: ${GROUP_ID}
      CONFIG_STORAGE_TOPIC: ${CONFIG_STORAGE_TOPIC}
      OFFSET_STORAGE_TOPIC: ${OFFSET_STORAGE_TOPIC}
      KEY_CONVERTER: ${KEY_CONVERTER}
      VALUE_CONVERTER: ${VALUE_CONVERTER}
      CONNECT_KEY_CONVERTER_SCHEMA_REGISTRY_URL: ${CONNECT_KEY_CONVERTER_SCHEMA_REGISTRY_URL}
      CONNECT_VALUE_CONVERTER_SCHEMA_REGISTRY_URL: ${CONNECT_VALUE_CONVERTER_SCHEMA_REGISTRY_URL}
    ports:
      - "8083:8083"
    depends_on:
      - kafka

  # -------------------- MINIO --------------------
  minio:
    image: minio/minio:latest
    container_name: minio
    hostname: minio
    mem_limit: 2g
    ports:
      - "9000:9000"
      - "9001:9001"
    environment:
      MINIO_ROOT_USER: ${MINIO_ROOT_USER}
      MINIO_ROOT_PASSWORD: ${MINIO_ROOT_PASSWORD}
    command: server /data --console-address ":9001"
    volumes:
      - ./minio_iceberg_data_lakehouse:/data

  # -------------------- MARKETING DW POSTGRES --------------------
  marketing_dw_postgres:
    image: postgres:16
    container_name: marketing_dw_postgres
    hostname: marketing_dw_postgres
    mem_limit: 2g
    ports:
      - "5432:5432"
    environment:
      POSTGRES_DB: ${POSTGRES_DB}
      POSTGRES_USER: ${POSTGRES_USER}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
    volumes:
      - ./postgres_data_warehouse:/var/lib/postgresql/data
    command:
      - "postgres"
      - "-c"
      - "wal_level=logical"
      - "-c"
      - "max_wal_senders=10"
      - "-c"
      - "max_replication_slots=10"
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${POSTGRES_USER} -d ${POSTGRES_DB}"]
      interval: 5s
      timeout: 5s
      retries: 5

  # -------------------- FLINK --------------------
  jobmanager:
    build: 
      context: .
      dockerfile: flink/docker/Dockerfile
    image: eczachly-pyflink
    pull_policy: never
    container_name: jobmanager
    hostname: jobmanager
    platform: linux/amd64
    mem_limit: 3g
    env_file:
      - ./flink/docker/flink-env.env
    expose:
      - "6123"
    ports:
      - "8081:8081"
    volumes:
      - ./flink/:/opt/src
    command: jobmanager
    extra_hosts:
      - "host.docker.internal:host-gateway"
    environment:
      - |
        FLINK_PROPERTIES=
        jobmanager.rpc.address: jobmanager

  taskmanager:
    image: eczachly-pyflink
    pull_policy: never
    container_name: taskmanager
    platform: linux/amd64
    mem_limit: 4g
    env_file:
      - ./flink/docker/flink-env.env
    expose:
      - "6121"
      - "6122"
    volumes:
      - ./flink/:/opt/src
    depends_on:
      - jobmanager
    command: taskmanager --taskmanager.registration.timeout 5 min
    extra_hosts:
      - "host.docker.internal:host-gateway"
    environment:
      - |
        FLINK_PROPERTIES=
        jobmanager.rpc.address: jobmanager
        taskmanager.numberOfTaskSlots: 15
        parallelism.default: 1


  # -------------------- POSTGRES SUPERSET --------------------
  postgres-superset:
    image: postgres:16
    container_name: postgres_superset
    hostname: postgres-superset
    mem_limit: 1.5g
    ports:
      - "5435:5432"
    environment:
      POSTGRES_DB: ${SUPERSET_DB}
      POSTGRES_USER: ${SUPERSET_USER}
      POSTGRES_PASSWORD: ${SUPERSET_PASSWORD}
    volumes:
      - ./superset/superset_metadata_db_data:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${SUPERSET_USER} -d ${SUPERSET_DB}"]
      interval: 5s
      timeout: 5s
      retries: 5

  # -------------------- REDIS --------------------
  redis:
    image: redis:7-alpine
    container_name: superset_redis
    mem_limit: 512m
    command: redis-server --appendonly yes
    ports:
      - "6379:6379"
    volumes:
      - ./superset/redis_data:/data
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 5s
      timeout: 3s
      retries: 5

  # -------------------- SUPERSET --------------------
  superset:
    build:
      context: .
      dockerfile: superset/docker/Dockerfile
    container_name: superset_app
    mem_limit: 1.5g
    ports:
      - "8088:8088"
    depends_on:
      postgres-superset:
        condition: service_healthy
      redis:
        condition: service_healthy
    environment:
      DATABASE_URL: postgresql+psycopg2://superset:superset@postgres-superset:5432/superset_metadata
      REDIS_HOST: superset_redis
      REDIS_PORT: 6379
      REDIS_CELERY_DB: 0
      REDIS_RESULTS_DB: 1
      REDIS_CACHE_DB: 2
      FLASK_APP: superset
      SUPERSET_LOAD_EXAMPLES: "no"
      SUPERSET_WEBSERVER_TIMEOUT: 60
    volumes:
      - ./superset/configuration/superset_config.py:/app/pythonpath/superset_config.py
    command: >
      sh -c "
        superset db upgrade &&
        superset init &&
        superset run -h 0.0.0.0 -p 8088 --with-threads
      "
    restart: always

  superset_worker:
    build:
      context: .
      dockerfile: superset/docker/Dockerfile
    container_name: superset_worker
    mem_limit: 1g
    depends_on:
      postgres-superset:
        condition: service_healthy
      redis:
        condition: service_healthy
    environment:
      DATABASE_URL: postgresql+psycopg2://superset:superset@postgres-superset:5432/superset_metadata
      REDIS_HOST: superset_redis
      REDIS_PORT: 6379
      REDIS_CELERY_DB: 0
      REDIS_RESULTS_DB: 1
      REDIS_CACHE_DB: 2
      FLASK_APP: superset
    volumes:
      - ./superset/configuration/superset_config.py:/app/pythonpath/superset_config.py
    command: celery --app=superset.tasks.celery_app:app worker --pool=prefork -O fair -c 4 --loglevel=info
    restart: always

  superset_beat:
    build:
      context: .
      dockerfile: superset/docker/Dockerfile
    container_name: superset_beat
    mem_limit: 1g
    depends_on:
      postgres-superset:
        condition: service_healthy
      redis:
        condition: service_healthy
    environment:
      DATABASE_URL: postgresql+psycopg2://superset:superset@postgres-superset:5432/superset_metadata
      REDIS_HOST: superset_redis
      REDIS_PORT: 6379
      REDIS_CELERY_DB: 0
      REDIS_RESULTS_DB: 1
      REDIS_CACHE_DB: 2
      FLASK_APP: superset
    volumes:
      - ./superset/configuration/superset_config.py:/app/pythonpath/superset_config.py
    command: >
      celery --app=superset.tasks.celery_app:app beat
      --pidfile /tmp/celerybeat.pid
      --schedule /tmp/celerybeat-schedule
      --loglevel=info
    restart: always

  # -------------------- POSTGRES AIRFLOW --------------------
  postgres_airflow:
    image: postgres:16
    container_name: postgres_airflow
    mem_limit: 1.5g
    ports:
      - "5436:5432"
    environment:
      POSTGRES_USER: ${AIRFLOW_USER}
      POSTGRES_PASSWORD: ${AIRFLOW_PASSWORD}
      POSTGRES_DB: ${AIRFLOW_DB}
    volumes:
      - ./airflow/postgres_data_airflow:/var/lib/postgresql/data

  # -------------------- AIRFLOW INIT --------------------
  airflow-init:
    build:
      context: .
      dockerfile: airflow/docker/Dockerfile
    mem_limit: 1g
    depends_on:
      - postgres_airflow
    environment:
      AIRFLOW__CORE__EXECUTOR: ${AIRFLOW_EXECUTOR}
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://${AIRFLOW_USER}:${AIRFLOW_PASSWORD}@postgres_airflow:5432/${AIRFLOW_DB}
      AIRFLOW__CORE__FERNET_KEY: ${AIRFLOW_FERNET_KEY}
      AIRFLOW__CORE__DEFAULT_TIMEZONE: ${AIRFLOW_TIMEZONE}
      AIRFLOW__CORE__LOAD_EXAMPLES: ${AIRFLOW_LOAD_EXAMPLES}
      AIRFLOW__SMTP__SMTP_HOST: ${SMTP_HOST}
      AIRFLOW__SMTP__SMTP_PORT: ${SMTP_PORT}
      AIRFLOW__SMTP__SMTP_STARTTLS: ${SMTP_STARTTLS}
      AIRFLOW__SMTP__SMTP_SSL: ${SMTP_SSL}
      AIRFLOW__SMTP__SMTP_USER: ${SMTP_USER}
      AIRFLOW__SMTP__SMTP_PASSWORD: ${SMTP_PASSWORD}
      AIRFLOW__SMTP__SMTP_MAIL_FROM: ${SMTP_MAIL_FROM}
    entrypoint: >
      bash -c "
        airflow db migrate &&
        airflow users create --username admin --password admin --firstname Admin --lastname User --role Admin --email ${SMTP_MAIL_FROM}
      "
    volumes:
      - ./airflow/dags:/opt/airflow/dags
      - ./airflow/airflow_logs:/opt/airflow/logs
      - /var/run/docker.sock:/var/run/docker.sock

  # -------------------- AIRFLOW WEBSERVER --------------------
  airflow-webserver:
    build:
      context: .
      dockerfile: airflow/docker/Dockerfile
    container_name: airflow-webserver
    mem_limit: 1.5g
    restart: always
    depends_on:
      - postgres_airflow
      - airflow-init
    ports:
      - "8080:8080"
    environment:
      AIRFLOW__CORE__EXECUTOR: ${AIRFLOW_EXECUTOR}
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://${AIRFLOW_USER}:${AIRFLOW_PASSWORD}@postgres_airflow:5432/${AIRFLOW_DB}
      AIRFLOW__CORE__FERNET_KEY: ${AIRFLOW_FERNET_KEY}
      AIRFLOW__CORE__DEFAULT_TIMEZONE: ${AIRFLOW_TIMEZONE}
      AIRFLOW__CORE__LOAD_EXAMPLES: ${AIRFLOW_LOAD_EXAMPLES}
      AIRFLOW__WEBSERVER__SESSION_BACKEND: securecookie
      AIRFLOW__SMTP__SMTP_HOST: ${SMTP_HOST}
      AIRFLOW__SMTP__SMTP_PORT: ${SMTP_PORT}
      AIRFLOW__SMTP__SMTP_STARTTLS: ${SMTP_STARTTLS}
      AIRFLOW__SMTP__SMTP_SSL: ${SMTP_SSL}
      AIRFLOW__SMTP__SMTP_USER: ${SMTP_USER}
      AIRFLOW__SMTP__SMTP_PASSWORD: ${SMTP_PASSWORD}
      AIRFLOW__SMTP__SMTP_MAIL_FROM: ${SMTP_MAIL_FROM}
    command: webserver
    volumes:
      - ./airflow/dags:/opt/airflow/dags
      - ./airflow/airflow_logs:/opt/airflow/logs
      - /var/run/docker.sock:/var/run/docker.sock

  # -------------------- AIRFLOW SCHEDULER --------------------
  airflow-scheduler:
    build:
      context: .
      dockerfile: airflow/docker/Dockerfile
    container_name: airflow-scheduler
    mem_limit: 1.5g
    restart: always
    depends_on:
      - airflow-webserver
    environment:
      AIRFLOW__CORE__EXECUTOR: ${AIRFLOW_EXECUTOR}
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://${AIRFLOW_USER}:${AIRFLOW_PASSWORD}@postgres_airflow:5432/${AIRFLOW_DB}
      AIRFLOW__CORE__FERNET_KEY: ${AIRFLOW_FERNET_KEY}
      AIRFLOW__CORE__DEFAULT_TIMEZONE: ${AIRFLOW_TIMEZONE}
      AIRFLOW__CORE__LOAD_EXAMPLES: ${AIRFLOW_LOAD_EXAMPLES}
      AIRFLOW__SMTP__SMTP_HOST: ${SMTP_HOST}
      AIRFLOW__SMTP__SMTP_PORT: ${SMTP_PORT}
      AIRFLOW__SMTP__SMTP_STARTTLS: ${SMTP_STARTTLS}
      AIRFLOW__SMTP__SMTP_SSL: ${SMTP_SSL}
      AIRFLOW__SMTP__SMTP_USER: ${SMTP_USER}
      AIRFLOW__SMTP__SMTP_PASSWORD: ${SMTP_PASSWORD}
      AIRFLOW__SMTP__SMTP_MAIL_FROM: ${SMTP_MAIL_FROM}
    command: scheduler
    volumes:
      - ./airflow/dags:/opt/airflow/dags
      - ./airflow/airflow_logs:/opt/airflow/logs
      - /var/run/docker.sock:/var/run/docker.sock

  # -------------------- DBT --------------------
  dbt:
    build:
      context: .
      dockerfile: dbt/docker/Dockerfile
    container_name: dbt
    mem_limit: 2g
    volumes:
      - ./dbt/dbt:/dbt
      - ./dbt/dbt/profiles.yml:/root/.dbt/profiles.yml
    working_dir: /dbt
    entrypoint: /bin/bash
    command: -c "tail -f /dev/null"
    depends_on:
      - clickhouse

  # -------------------- PYSPARK --------------------
  pyspark:
    build: 
      context: .
      dockerfile: spark/docker/Dockerfile
    container_name: pyspark
    hostname: pyspark
    mem_limit: 12g
    environment:
      - SPARK_MODE=client
      - SPARK_LOCAL_DIRS=/tmp/spark
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_RPC_ENCRYPTION_ENABLED=no
      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
      - SPARK_SSL_ENABLED=no
      - SPARK_UI_PORT=4040
      - SPARK_UI_BIND_ADDRESS=0.0.0.0
      - SPARK_DRIVER_MEMORY=13g
    ports:
      - "4040:4040"
    volumes:
      - ./spark:/opt/spark
      - ./src/raw_data:/opt/raw_data
    command: tail -f /dev/null
    depends_on:
      - minio
      - marketing_dw_postgres
    extra_hosts:
      - "host.docker.internal:host-gateway"

  # -------------------- CLICKHOUSE --------------------
  clickhouse:
    image: clickhouse/clickhouse-server:24.3.8
    container_name: clickhouse
    hostname: clickhouse
    ports:
      - "8123:8123"
      - "9005:9000"
    environment:
      CLICKHOUSE_USER: ${CLICKHOUSE_USER}
      CLICKHOUSE_PASSWORD: ${CLICKHOUSE_PASSWORD}
    volumes:
      - clickhouse_data:/var/lib/clickhouse
      - ./src/raw_data:/var/lib/clickhouse/user_files/raw_data
    depends_on:
      - zookeeper
      - kafka

# -------------------- VOLUMES --------------------
volumes:
  kafka_store:
  kafak_secrets:
  minio_iceberg_data_lakehouse:
  postgres_data_warehouse:
  flink:
  redis_data:
  superset_metadata_db_data:
  postgres_data_airflow:
  dbt:
  spark:
  clickhouse_data:
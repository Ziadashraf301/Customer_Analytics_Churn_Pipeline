# ğŸ“Š Customer Analytics And Churn Pipeline: Real-Time & Batch Data Pipeline

![intro\_image](images/intro_image.jpg)

## ğŸ’¼ Business Problem

Customer churn is one of the biggest challenges in marketing and growth. Companies often spend **5â€“7x more** to acquire a new customer than to retain an existing one. Without timely insights, businesses struggle to:

* Detect early signs of customer churn
* Segment customers for personalized marketing
* Track real-time customer behavior (website visits, purchases)

---

## ğŸ† How This Project Helps the Business

This pipeline directly addresses these problems by:

* âš¡ **Real-time monitoring** of customer interactions (web events, purchases) to quickly spot drops in engagement
* ğŸ¯ Customer segmentation via clustering, enabling churn risk identification, targeted campaigns, upselling, and personalized offers
* ğŸ“Š **Dashboards for decision-makers** with both real-time KPIs (page views, product clicks, orders) and historical trends (monthly acquisitions, country performance)
* ğŸ’° **Optimized retention strategies** that reduce churn, increase lifetime value, and improve ROI of marketing campaigns

---

## ğŸš€ Overview

This project demonstrates a **hybrid data pipeline** that combines **real-time streaming** and **batch transformations** for advanced customer analytics and churn prediction.

* **Streaming** â†’ Python simulations generate **website visits & purchases**, ingested into **Kafka**. **Flink** consumes events, processes them in real time, performs **aggregations**, and stores results in **Postgres**.
* **Batch / dbt** â†’ dbt transforms Postgres data into **staging**, **intermediate**, and **mart** layers. **ClickHouse** serves as the analytical warehouse, with **Debezium CDC** syncing Postgres â†’ ClickHouse.
* **ML & Spark** â†’ Data flows from ClickHouse/dbt â†’ **Parquet on MinIO** â†’ **Iceberg tables** â†’ **Spark ML K-Means** clustering â†’ **customer segments**. Cluster results are analyzed further in **Jupyter Notebook**.
* **Superset** â†’ Provides **interactive dashboards** with both **real-time metrics** and **historical KPIs**.
* **Airflow** â†’ Orchestrates ingestion, dbt transformations, and ML workflows.
* **Docker** â†’ Every service (Kafka, Flink, Postgres, ClickHouse, Superset, Airflow, MinIO) runs inside **Docker containers**.
* **GitHub CI/CD** â†’ Full codebase, workflows, and docs are maintained in GitHub for collaboration and automation.

---

## ğŸ¯ Key Features

- âœ… Hybrid data pipeline (streaming + batch)
- âœ… Real-time ingestion with Kafka & Flink
- âœ… dbt transformations with **incremental models**
- âœ… ClickHouse for ultra-fast analytics
- âœ… ML clustering with Spark & Iceberg
- âœ… Automated orchestration with Airflow
- âœ… BI dashboards with Superset
- âœ… Dockerized environment for easy setup

---

## ğŸ“Š Results & Impact

* âš¡ **High throughput** â†’ Handles **10k events/sec (\~36M per hour)** with sub-second latency
* ğŸ›¢ï¸ **Optimized storage** â†’ Postgres, ClickHouse, Iceberg + compression for efficiency
* ğŸ“‰ **Low query latency** â†’ Vectorized queries in ClickHouse for instant aggregations
* ğŸ§© **Incremental models** â†’ dbt avoids costly full reloads
* ğŸ“ˆ **Adoption-ready dashboards** â†’ Superset tracks **real-time** + **historical KPIs**
* ğŸ¤– **ML insights** â†’ Spark K-Means clusters customers into actionable groups with **95%+ pipeline reliability**
* ğŸ”„ **Automation** â†’ Airflow DAGs ensure ingestion, transformation, and ML retraining run seamlessly
* â° **Fresh data** â†’ Pipelines orchestrated every **6 hours**
* ğŸ“§ **Monitoring** â†’ Automatic alerts + summary reports for stakeholders via Emails

---

## ğŸ—ï¸ Architecture

![architecture](images/Data_Pipeline.png)

### Streaming Pipeline

1. **Simulated events** â†’ Kafka
2. **Kafka â†’ Flink â†’ Postgres**
3. **Flink performs aggregations** â†’ stored in Postgres

### Batch / dbt Pipeline

1. Hybrid data pipeline:
   - *Customers Profiles â†’ ClickHouse*
   - *Postgres â†’ Debezium CDC â†’ Kafka â†’ ClickHouse*

2. **dbt** runs transformations:

   * *Staging* â†’ Clean & standardize
   * *Intermediate* â†’ Apply business rules
   * *Marts* â†’ Funnel, ML, Monthly Registration Model, Country Model

3. **ML Pipeline**:

   * ClickHouse marts â†’ Parquet â†’ MinIO â†’ Iceberg
   * Spark ML runs **K-Means** â†’ stores clusters & models
   * Jupyter Notebook â†’ deeper analysis of **customer clusters**

---

## ğŸ”„ Airflow DAGs

* **Pipeline DAG** â†’ dbt + ML workflow

![pipeline\_dags](images/pipeline_dags.png)

* **Cluster DAG** â†’ ML clustering jobs

![cluster\_dags](images/clusters.png)

* **Stream DAG** â†’ Flink streaming

![stream\_dags](images/kafka_flink_streams.png)

---

## ğŸ› ï¸ Tech Stack

* **Kafka** â†’ Event streaming
* **Flink** â†’ Real-time aggregations
* **Postgres** â†’ Source + Row Streams Storege
* **Debezium** â†’ CDC from Postgres â†’ Kafka â†’ ClickHouse
* **ClickHouse** â†’ Analytical warehouse
* **dbt** â†’ Batch transformations
* **Parquet / MinIO** â†’ Data lake storage
* **Apache Spark + Iceberg** â†’ ML-ready tables + clustering
* **Superset** â†’ BI dashboards
* **Airflow** â†’ Orchestration
* **Docker Compose** â†’ Environment setup
* **Github** â†’ CI/CD

---

## ğŸ“Œ How to Run the Project

### 1. Clone the repo

```bash
git clone https://github.com/Ziadashraf301/Customer_Analytics_Churn_Pipeline.git
cd Customer_Analytics_Churn_Pipeline
```

### 2. Start services

```bash
docker-compose up -d
```

### 3. Create schemas and tables

Connect to the databases running inside the containers and manually execute the DDL statements. The SQL commands are provided in the `sql_queries/` directory â€” copy them into your database clients.

* **Postgres**

  ```bash
  docker exec -it marketing_dw_postgres psql -U user -d marketing_dw
  ```

  Then copy the SQL statements from **`sql_queries/postgres_ddl.sql`** and paste them into the psql session.

* **ClickHouse**

  ```bash
   docker exec -it clickhouse clickhouse-client --user default
  ```

  Then copy the SQL statements from **`sql_queries/clickhouse_.sql`** files and paste them into the ClickHouse client.


### 4. Generate batch data

```bash
python src/data_generation_scripts/generate_master_customer_ids.py
python src/data_generation_scripts/generate_batch_customers_profile_data.py
```

### 5. Generate streaming events

```bash
make generate_streaming_purchases
make generate_web_events
```

### 6. Run Debezium connectors

Register **website events** + **purchase events** CDC connectors:

```bash
# Website events connector
curl -X POST http://localhost:8083/connectors \
-H "Content-Type: application/json" \
-d @src/debezium/web_event_connector.json

# Purchase events connector
curl -X POST http://localhost:8083/connectors \
-H "Content-Type: application/json" \
-d @src/debezium/purchase_events_connector.json
```

### 7. Trigger Airflow DAGs

Access the Airflow UI at: http://localhost:8080

### 8. Explore dashboards

Access the Superset at http://localhost:8088

---

## ğŸ”Œ Ports

| Service             | Port  |
| ------------------- | ----- |
| Airflow Web UI      | 8080  |
| Superset            | 8088  |
| Postgres (DW)       | 5432  |
| Postgres (Superset) | 5435  |
| Postgres (Airflow)  | 5436  |
| ClickHouse          | 8123  |
| Spark UI            | 4040  |
| Kafka Broker        | 9092  |
| Kafdrop UI          | 19000 |
| Flink Web UI        | 8081  |
| MinIO               | 9000  |

---

## ğŸ“Š BI Dashboards

* **Real-Time Metrics** (Flink + Postgres)

![](images/website-tracking.jpg)
![](images/orders-tracking.jpg)

* **Country & Monthly Reports** (ClickHouse via dbt)

![](images/the-market-by-country.jpg)
![](images/customers-aquesition.jpg)

---

## ğŸ¤– Machine Learning

* **K-Means clustering** â†’ actionable segments
* **Feature Store** â†’ Iceberg tables
* **Jupyter Notebooks** â†’ customer journey & churn insights
* **Outputs** â†’ cluster tables + ML models

---

## ğŸ“š Documentation

* [dbt Models](dbt/dbt/models/)
* [Airflow DAGs](airflow/dags/)
* [Spark ML Analysis](python_analysis/)

---
